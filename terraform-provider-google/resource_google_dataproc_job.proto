syntax "proto3";

package google;

message GoogleDataprocJob {

  // Output-only. If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
  driver_controls_files_uri = 1;

  // Output-only. A URI pointing to the location of the stdout of the job's driver program
  driver_output_resource_uri = 2;
  force_delete = 3;
  id = 4;

  // Optional. The labels to associate with this job.
  labels = 5;
  project = 6;
  region = 7;
  status = 8;
  message HadoopConfig {
    archive_uris = 1;
    args = 2;
    file_uris = 3;
    jar_file_uris = 4;
    main_class = 5;
    main_jar_file_uri = 6;
    properties = 7;
    message LoggingConfig {

      // Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
      driver_log_levels = 1;
    }
    logging_config = 8;
  }
  hadoop_config = 9;
  message HiveConfig {
    continue_on_failure = 1;
    jar_file_uris = 2;
    properties = 3;
    query_file_uri = 4;
    query_list = 5;
    script_variables = 6;
  }
  hive_config = 10;
  message PigConfig {
    continue_on_failure = 1;
    jar_file_uris = 2;
    properties = 3;
    query_file_uri = 4;
    query_list = 5;
    script_variables = 6;
    message LoggingConfig {

      // Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
      driver_log_levels = 1;
    }
    logging_config = 7;
  }
  pig_config = 11;
  message Placement {

    // The name of the cluster where the job will be submitted
    cluster_name = 1;

    // Output-only. A cluster UUID generated by the Cloud Dataproc service when the job is submitted
    cluster_uuid = 2;
  }
  placement = 12;
  message PysparkConfig {

    // Optional. HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip
    archive_uris = 1;

    // Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission
    args = 2;

    // Optional. HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks
    file_uris = 3;

    // Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks
    jar_file_uris = 4;

    // Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file
    main_python_file_uri = 5;

    // Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code
    properties = 6;

    // Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip
    python_file_uris = 7;
    message LoggingConfig {

      // Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
      driver_log_levels = 1;
    }
    logging_config = 8;
  }
  pyspark_config = 13;
  message Reference {

    // The job ID, which must be unique within the project. The job ID is generated by the server upon job submission or provided by the user as a means to perform retries without creating duplicate jobs
    job_id = 1;
  }
  reference = 14;
  message Scheduling {

    // Maximum number of times per hour a driver may be restarted as a result of driver terminating with non-zero code before job is reported failed.
    max_failures_per_hour = 1;
  }
  scheduling = 15;
  message SparkConfig {
    archive_uris = 1;
    args = 2;
    file_uris = 3;
    jar_file_uris = 4;
    main_class = 5;
    main_jar_file_uri = 6;
    properties = 7;
    message LoggingConfig {

      // Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
      driver_log_levels = 1;
    }
    logging_config = 8;
  }
  spark_config = 16;
  message SparksqlConfig {
    jar_file_uris = 1;
    properties = 2;
    query_file_uri = 3;
    query_list = 4;
    script_variables = 5;
    message LoggingConfig {

      // Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
      driver_log_levels = 1;
    }
    logging_config = 6;
  }
  sparksql_config = 17;
  message Timeouts {
    create = 1;
    delete = 2;
  }
  timeouts = 18;
}